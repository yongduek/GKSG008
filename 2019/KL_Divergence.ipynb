{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture Note on KL Divergence\n",
    "\n",
    "yndk@sogang.ac.kr\n",
    "\n",
    "Mainly from\n",
    "**Geometric Modeling in Probability and Statistics, Cilin and Udrste, 2014**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kullback-Leibler Divergence\n",
    "The Kullbackâ€“Leibler divergence was introduced by Solomon Kullback and Richard Leibler in 1951 as the directed divergence between two distributions; Kullback preferred the term discrimination information. The divergence is discussed in Kullback's 1959 book, Information Theory and Statistics [wikipedia.org]\n",
    "\n",
    "- Also known as the Kullback-Leibler Relative Entropy\n",
    "\n",
    "- The KL relative entropy is a non-commutative measure of the difference between two probability densities $p$ and $q$ on the same statistical manifold.\n",
    "    - Definition:\n",
    "\n",
    "    $$\n",
    "        D_{KL}(p||q) = \\mathbb{E}_p\\big[ \\log \\frac{p}{q} \\big] = \\sum_{i \\in X} p_i \\log\\frac{p_i}{q_i}\n",
    "    $$\n",
    "\n",
    "- In information theory the density $p$ is considered to be the true density (normally unknown), while $q$ is the theoretical model density.\n",
    "- The KL relative entropy may be regarded as a measure of inefficiency of assuming data distributed according to $q$, when actually it is distributed as $p$.\n",
    "\n",
    "## Entropy\n",
    "- The entropy of a discrete density $p=\\{p_1, ..., p_n\\}$ is defined to be:\n",
    "\n",
    "$$\n",
    "    H(p) = \\sum_i p_i \\log\\frac{1}{p_i} = -\\sum_i p_i\\log p_i\n",
    "$$\n",
    "\n",
    "## Cross Entropy\n",
    "- The cross entropy $H(p,q)$ of two densities $p$ and $q$ is defined to be\n",
    "\n",
    "$$\n",
    "    H(p, q) = \\sum_i p_i \\log\\frac{1}{q_i} = -\\sum_i p_i\\log q_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemma: $D_{KL}(p||q) \\geq 0$\n",
    "\n",
    "If $p=\\{p_1, ..., p_n\\}$ and $q=\\{ q_1, ..., q_n \\}$ are two strictly positive discrete densities on the same event sapce. Then\n",
    "\n",
    "$$    \n",
    "    \\begin{align}\n",
    "        H(p,p)  \\leq H(p,q) &\\quad\\Leftrightarrow\\quad\n",
    "        -\\sum_i p_i\\log p_i  \\leq -\\sum_i p_i\\log q_i\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "Or\n",
    "\n",
    "$$    \n",
    "    \\begin{align}\n",
    "        \\sum_i p_i\\log p_i  \\geq \\sum_i p_i\\log q_i\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "**Proof** Using the inequality $\\log x \\leq x - 1$ (prove?) for  $x>0$, we find\n",
    "\n",
    "$$\n",
    "    \\begin{align}\n",
    "        \\sum_i p_i \\log q_i - \\sum_i p_i \\log p_i \n",
    "            = \\sum_i p_i \\log\\frac{q_i}{p_i}\n",
    "            & \\leq \\sum_i p_i \\left( \\frac{q_i}{p_i} - 1 \\right) \\\\\n",
    "            & = \\sum_i q_i - \\sum_i p_i = 0\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "The equality is reached for $q_i/p_i = 1$, i.e., the case of equal densities.\n",
    "Therefore\n",
    "\n",
    "$$\n",
    "    D_{KL}(p||q) = -\\sum_i p_i \\log q_i + \\sum_i p_i \\log p_i \\geq 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposition\n",
    "\n",
    "The relative entropy $D_{KL}(p||q)$, the entropy $H(p)$ and the cross entropy $H(p,q)$ are related by\n",
    "\n",
    "$$\n",
    "    D_{KL} = H(p,q) - H(p)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corollary\n",
    "The entropy $H(p)$ and the cross entropy $H(p,q)$ satisfy the inequality\n",
    "\n",
    "$$\n",
    "    H(p) \\leq H(p,q)\n",
    "$$\n",
    "\n",
    "with equality if and only if $p = q$.\n",
    "\n",
    "* This shows that the cross entropy is the minimum when the two distributions are identical.\n",
    "\n",
    "* This can also be stated as:\n",
    "\n",
    "$$\n",
    "    \\min_q H(p,q) = H(p)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth noting that the KL relative entropy can be also written as a difference of two log-likelihood functions:\n",
    "\n",
    "$$\n",
    "    D_{KL}(p||q) = \\mathbb{E}_p[l_p] - \\mathbb{E}_p[l_q]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KL Divergence of Two Gaussian Distributions\n",
    "Let us consider two Gaussian distributions $p$ and $q$:\n",
    "\n",
    "$$\n",
    "p(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma_1} \\exp \\bigg[ -\\frac{(x-\\mu_1)^2}{2\\sigma_1^2} \\bigg]\n",
    "\\quad\\mbox{and}\\quad\n",
    "q(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma_2} \\exp \\bigg[ -\\frac{(x-\\mu_2)^2}{2\\sigma_2^2} \\bigg]\n",
    "$$\n",
    "\n",
    "The KL divergence of $p$ and $q$ \n",
    "$$\n",
    "    D_{KL}(p||q) = \\frac{1}{2}\n",
    "                    \\bigg[\n",
    "                        \\big( \\frac{\\sigma_1}{\\sigma_2} \\big)^2\n",
    "                        - \n",
    "                        \\ln\\big( \\frac{\\sigma_1}{\\sigma_2} \\big)^2\n",
    "                        -\n",
    "                        1\n",
    "                    \\bigg]\n",
    "                    +\n",
    "                    \\frac{(\\mu_1 - \\mu_2)^2}{2\\sigma_2^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot $D_{KL}(p||q)$ when $p(x) = \\mathcal{N}(x|0,1)$. It is a function of $\\sigma_2$ and $\\mu_2$. Do the same when $q(x) = \\mathcal{N}(x|0,1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KL Diverence of two multivariate Gaussians\n",
    "\n",
    "$$\n",
    "    D_{KL}(p||q) = \\frac{1}{2}\\bigg[\n",
    "                        \\ln\\frac{|\\Sigma_2|}{|\\Sigma_1|}\n",
    "                        - k\n",
    "                        + \\mathrm{trace}\\big(\\Sigma_2^{-1}\\Sigma_1\\big)\n",
    "                        +\n",
    "                        \\big(\\mu_2 - \\mu_1\\big)^\\top\\Sigma_2^{-1}\\big(\\mu_2 - \\mu_1\\big)\n",
    "                    \\bigg]\n",
    "$$\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Multivariate_normal_distributions\n",
    "- https://stats.stackexchange.com/questions/60680/kl-divergence-between-two-multivariate-gaussians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KL Divergence when $q(x)=\\mathcal{N}(x|0,I)$\n",
    "\n",
    "A special case, and a common quantity in variational inference, is the KL-divergence between a diagonal multivariate normal, and a standard normal:\n",
    "\n",
    "$$\n",
    "    \\begin{align}\n",
    "        \\mu & = \\big(\\mu_1, ..., \\mu_k\\big)  \\\\\n",
    "        \\sigma & = \\big(\\sigma_1, ..., \\sigma_k\\big)\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    D_{KL}\\big(\\mathcal{N}(\\mu, \\mathrm{diag}(\\sigma_1,...,\\sigma_k) \\big|\\big| \\mathcal(0,I)\\big)\n",
    "    =\n",
    "    \\frac{1}{2}\\sum_{i=1}^k \\bigg(\\sigma_i^2 + \\mu_i^2 - \\ln\\sigma_i^2 - 1 \\bigg)\n",
    "$$\n",
    "\n",
    "**Prove**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Bishop\n",
    "- MacKay\n",
    "- Murphy\n",
    "- Geometric Modeling in Probability and Statistics, Cilin and Udrste, 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
